{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal echoing\n",
    "\n",
    "Echoing signal `n` steps is an example of synchronized many-to-many task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequential_tasks import EchoData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# By taking away these seeds, we see that even performance here is very stochastic.\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "# np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "echo_step = 3\n",
    "series_length = 20_000\n",
    "BPTT_T = 20\n",
    "\n",
    "# EchoData provides input and target data for training a network to\n",
    "# echo a `series_length`-long stream of data. `.x_batch` contains the input series,\n",
    "# it has shape `[batch_size, series_length]`; `.y_batch` contains the target data,\n",
    "# it has the same shape as `.x_batch`.\n",
    "#\n",
    "# Unlike other training data in this course, successive batches from a single `EchoData`\n",
    "# object draw from the same stream. For example, in 08-seq_classification, training data\n",
    "# has the following format:\n",
    "#\n",
    "#   [[S11 S12...S1N], [S21 S22...S2N], ..., [SM1 SM2...SMN]]\n",
    "#\n",
    "# where `SIJ` represents the `j`th sample drawn from the `i`th stream. \n",
    "#\n",
    "# However, `EchoData` output has the following format (slicing along the batch dimension):\n",
    "#\n",
    "#   [[S11 S21...S1N], [S1(N+1) S1(N+2)...S2(2N)], ..., [S1(MN) S1(MN+1)...SM(MNN)]]\n",
    "#\n",
    "# This means that successive batches of data drawn from the same `EchoData` object\n",
    "# are not independent.\n",
    "train_data = EchoData(\n",
    "    echo_step=echo_step,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    truncated_length=BPTT_T\n",
    ")\n",
    "total_values_in_one_chunck = batch_size * BPTT_T\n",
    "train_size = len(train_data)\n",
    "\n",
    "test_data = EchoData(\n",
    "    echo_step=echo_step,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    truncated_length=BPTT_T,\n",
    ")\n",
    "test_data.generate_new_series()\n",
    "test_data.prepare_batches()\n",
    "test_size = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1st input sequence)  x: 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 ... \n",
      "(1st target sequence) y: 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 ... \n",
      "(1st input sequence)  x: 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 ... \n",
      "(1st target sequence) y: 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 ... \n"
     ]
    }
   ],
   "source": [
    "# Let's print first 20 timesteps of the first sequences to see the echo data:\n",
    "print('(1st input sequence)  x:', *train_data.x_batch[0, :20], '... ')\n",
    "print('(1st target sequence) y:', *train_data.y_batch[0, :20], '... ')\n",
    "\n",
    "print('(1st input sequence)  x:', *test_data.x_batch[0, :20], '... ')\n",
    "print('(1st target sequence) y:', *test_data.y_batch[0, :20], '... ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch:\n",
      "1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 ...\n",
      "0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 ...\n",
      "1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 ...\n",
      "0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 ...\n",
      "1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 ...\n",
      "x_batch size: (5, 20000)\n",
      "\n",
      "y_batch:\n",
      "0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 ...\n",
      "0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 ...\n",
      "0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 ...\n",
      "0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0 ...\n",
      "0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 ...\n",
      "y_batch size: (5, 20000)\n"
     ]
    }
   ],
   "source": [
    "# batch_size different sequences are created:\n",
    "print('x_batch:', *(str(d)[1:-1] + ' ...' for d in train_data.x_batch[:, :20]), sep='\\n')\n",
    "print('x_batch size:', train_data.x_batch.shape)\n",
    "print()\n",
    "print('y_batch:', *(str(d)[1:-1] + ' ...' for d in train_data.y_batch[:, :20]), sep='\\n')\n",
    "print('y_batch size:', train_data.y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_chunk:\n",
      "[1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0]\n",
      "[0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0]\n",
      "[1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1]\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0]\n",
      "[1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0]\n",
      "1st x_chunk size: (5, 20, 1)\n",
      "\n",
      "y_chunk:\n",
      "[0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1]\n",
      "[0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0]\n",
      "[0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1]\n",
      "[0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0]\n",
      "[0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1]\n",
      "1st y_chunk size: (5, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "# In order to use RNNs data is organized into temporal\n",
    "# chunks of size [batch_size, T, feature_dim]\n",
    "print('x_chunk:', *train_data.x_chunks[0].squeeze(), sep='\\n')\n",
    "print('1st x_chunk size:', train_data.x_chunks[0].shape)\n",
    "print()\n",
    "print('y_chunk:', *train_data.y_chunks[0].squeeze(), sep='\\n')\n",
    "print('1st y_chunk size:', train_data.y_chunks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, rnn_hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = torch.nn.RNN( # This generates a series of encodings\n",
    "            input_size=input_size,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=1, # Deep RNNs have a stack of hidden representations.\n",
    "            nonlinearity='relu', # Also, why isn't there a weighting on the losses,\n",
    "            # Such that the losses from later rounds are weighed more than\n",
    "            # earlier ones, because some sequences start without any context, and\n",
    "            # this just creates noise.\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = torch.nn.Linear( # This is the decoder\n",
    "            in_features=rnn_hidden_size,\n",
    "            out_features=output_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # In order to model the fact that successive batches belong to the same stream of data,\n",
    "        # we share the hidden state across successive invocations.\n",
    "        rnn_out, hidden = self.rnn(x, hidden)  \n",
    "        if self.training:\n",
    "            rnn_out.retain_grad()\n",
    "        out = self.linear(rnn_out)\n",
    "        return out, hidden, rnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0 torch.Size([4, 1])\n",
      "rnn.weight_hh_l0 torch.Size([4, 4])\n",
      "rnn.bias_ih_l0 torch.Size([4])\n",
      "rnn.bias_hh_l0 torch.Size([4])\n",
      "linear.weight torch.Size([1, 4])\n",
      "linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "m = SimpleRNN(\n",
    "    input_size=1,\n",
    "    rnn_hidden_size=4,\n",
    "    output_size=1\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, param in m.named_parameters():\n",
    "        print(name, param.shape)\n",
    "# Corresponds to ReLU(W_ih x_t + W_hh h_t-1 + b_ih + b_hh)\n",
    "# And there are two biases just for compatibility with LSTM/GRU\n",
    "\n",
    "# For the linear weights, the gradient computation is straightforward\n",
    "# There is no problem with optimization there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    # New epoch --> fresh hidden state\n",
    "    hidden = None   \n",
    "    correct = 0\n",
    "    for batch_idx in range(train_size):\n",
    "        data, target = train_data[batch_idx]\n",
    "        data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if hidden is not None: hidden.detach_()\n",
    "        logits, hidden, _ = model(data, hidden)\n",
    "\n",
    "        # RNN has a bijection between \n",
    "        # print(data.shape, target.shape)\n",
    "\n",
    "        loss = criterion(logits, target) # It doesn't do anything really special\n",
    "        # It just has a 1-1 mapping between data and target, all at once.\n",
    "        # And then it later on does a topological sort, rooted at loss.\n",
    "        loss.backward() # Calculates all gradients involved (anything that has autograd=True)\n",
    "        # And adds the grad dL/dw_i\n",
    "        optimizer.step() # This just steps all of those things, but carefully (i.e. following\n",
    "        # a stepping algorithm, like AdamW)\n",
    "\n",
    "        \n",
    "        pred = (torch.sigmoid(logits) > 0.5)\n",
    "        correct += (pred == target.byte()).int().sum().item()/total_values_in_one_chunck\n",
    "        \n",
    "    return correct, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()   \n",
    "    correct = 0\n",
    "    # New epoch --> fresh hidden state\n",
    "    hidden = None\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(test_size):\n",
    "            data, target = test_data[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).float().to(device)\n",
    "            logits, hidden, _ = model(data, hidden)\n",
    "            \n",
    "            pred = (torch.sigmoid(logits) > 0.5)\n",
    "            correct += (pred == target.byte()).int().sum().item()/total_values_in_one_chunck\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 1 #since we have a scalar series\n",
    "h_units = 4\n",
    "\n",
    "model = SimpleRNN(\n",
    "    input_size=1,\n",
    "    rnn_hidden_size=h_units,\n",
    "    output_size=feature_dim\n",
    ").to(device)\n",
    "        \n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/5, loss: 0.466, accuracy 71.9%\n",
      "Train Epoch: 2/5, loss: 0.244, accuracy 84.7%\n",
      "Train Epoch: 3/5, loss: 0.410, accuracy 85.6%\n",
      "Train Epoch: 4/5, loss: 0.353, accuracy 85.7%\n",
      "Train Epoch: 5/5, loss: 0.392, accuracy 85.2%\n",
      "Test accuracy: 85.3%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    correct, loss = train()\n",
    "    train_accuracy = float(correct)*100/ train_size\n",
    "    print(f'Train Epoch: {epoch}/{n_epochs}, loss: {loss:.3f}, accuracy {train_accuracy:.1f}%')\n",
    "\n",
    "#test    \n",
    "correct = test()\n",
    "test_accuracy = float(correct) * 100 / test_size\n",
    "print(f'Test accuracy: {test_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "         1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "         1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "         1, 0, 1, 0]], dtype=torch.uint8)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "         1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "         1, 0, 0, 0]], dtype=torch.uint8)\n",
      "[ True  True  True  True False  True  True False False  True  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False  True  True  True False  True\n",
      "  True  True  True  True  True False False False  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True False False False  True False  True  True  True  True  True\n",
      "  True  True  True False]\n"
     ]
    }
   ],
   "source": [
    "# Let's try some echoing\n",
    "my_input = torch.empty(1, 100, 1).random_(2)\n",
    "hidden = None\n",
    "my_out, _, _ = model(my_input.to(device), hidden)\n",
    "my_pred = torch.where(my_out > .5, \n",
    "                      torch.ones_like(my_out), \n",
    "                      torch.zeros_like(my_out)).cpu()\n",
    "print(my_input.view(1, -1).byte(), my_pred.view(1, -1).byte(), sep='\\n')\n",
    "\n",
    "# Calculate the expected output for our random input\n",
    "expected = np.roll(my_input, echo_step)\n",
    "expected[:, :echo_step] = 0\n",
    "correct = expected == my_pred.numpy()\n",
    "print(np.ndarray.flatten(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20, 4]) tensor([[[1.4433e-01, 0.0000e+00, 0.0000e+00, 1.9459e+00],\n",
      "         [4.0521e+00, 0.0000e+00, 0.0000e+00, 2.3875e+00],\n",
      "         [5.8795e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [4.3304e+00, 1.5520e+00, 0.0000e+00, 7.4895e-01],\n",
      "         [3.2438e+00, 1.4465e+00, 0.0000e+00, 1.6397e+00],\n",
      "         [3.5931e+00, 8.8189e-03, 0.0000e+00, 1.3393e+00],\n",
      "         [5.7784e+00, 0.0000e+00, 0.0000e+00, 2.0017e+00],\n",
      "         [8.1300e+00, 7.6921e-01, 0.0000e+00, 1.2120e+00],\n",
      "         [6.9444e+00, 2.8012e+00, 0.0000e+00, 1.4517e-01],\n",
      "         [5.1594e+00, 5.4392e+00, 0.0000e+00, 3.6572e+00],\n",
      "         [5.1073e+00, 3.4174e+00, 0.0000e+00, 2.3831e+00],\n",
      "         [4.6927e+00, 2.4586e+00, 0.0000e+00, 1.7971e+00],\n",
      "         [5.7221e+00, 2.4878e+00, 0.0000e+00, 3.0574e+00],\n",
      "         [7.9431e+00, 2.2468e+00, 0.0000e+00, 2.2200e+00],\n",
      "         [7.1790e+00, 3.3029e+00, 0.0000e+00, 5.7989e-01],\n",
      "         [5.5617e+00, 5.7312e+00, 0.0000e+00, 3.6968e+00],\n",
      "         [6.7577e+00, 4.6567e+00, 0.0000e+00, 3.9296e+00],\n",
      "         [8.4950e+00, 4.2587e+00, 0.0000e+00, 2.9027e+00],\n",
      "         [8.7310e+00, 5.7467e+00, 0.0000e+00, 2.8293e+00],\n",
      "         [6.4879e+00, 6.5722e+00, 0.0000e+00, 2.2704e+00]],\n",
      "\n",
      "        [[1.4433e-01, 0.0000e+00, 0.0000e+00, 1.9459e+00],\n",
      "         [4.0521e+00, 0.0000e+00, 0.0000e+00, 2.3875e+00],\n",
      "         [7.3615e+00, 0.0000e+00, 0.0000e+00, 1.3657e+00],\n",
      "         [7.0158e+00, 1.4945e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [4.2941e+00, 3.6459e+00, 0.0000e+00, 1.4413e+00],\n",
      "         [4.3421e+00, 3.5853e+00, 0.0000e+00, 4.0547e+00],\n",
      "         [7.5304e+00, 1.6499e+00, 0.0000e+00, 2.6682e+00],\n",
      "         [7.7579e+00, 2.1650e+00, 0.0000e+00, 6.5249e-02],\n",
      "         [6.0031e+00, 5.4297e+00, 0.0000e+00, 3.1394e+00],\n",
      "         [6.5770e+00, 5.0673e+00, 0.0000e+00, 3.9391e+00],\n",
      "         [6.6634e+00, 3.7697e+00, 0.0000e+00, 1.7203e+00],\n",
      "         [4.8100e+00, 4.2545e+00, 0.0000e+00, 2.0377e+00],\n",
      "         [3.5952e+00, 3.2913e+00, 0.0000e+00, 2.5520e+00],\n",
      "         [5.3710e+00, 2.0040e+00, 0.0000e+00, 3.4086e+00],\n",
      "         [8.3860e+00, 1.3229e+00, 0.0000e+00, 1.8125e+00],\n",
      "         [7.5305e+00, 3.0333e+00, 0.0000e+00, 1.2751e-01],\n",
      "         [5.4243e+00, 6.0374e+00, 0.0000e+00, 3.6904e+00],\n",
      "         [6.4792e+00, 4.8553e+00, 0.0000e+00, 4.1501e+00],\n",
      "         [8.4475e+00, 4.1032e+00, 0.0000e+00, 2.9692e+00],\n",
      "         [8.8645e+00, 5.5251e+00, 0.0000e+00, 2.7088e+00]],\n",
      "\n",
      "        [[1.4433e-01, 0.0000e+00, 0.0000e+00, 1.9459e+00],\n",
      "         [4.0521e+00, 0.0000e+00, 0.0000e+00, 2.3875e+00],\n",
      "         [7.3615e+00, 0.0000e+00, 0.0000e+00, 1.3657e+00],\n",
      "         [8.4978e+00, 2.2391e+00, 0.0000e+00, 1.2156e+00],\n",
      "         [7.8612e+00, 5.1266e+00, 0.0000e+00, 2.4450e+00],\n",
      "         [7.2425e+00, 6.4760e+00, 0.0000e+00, 3.7293e+00],\n",
      "         [6.0900e+00, 5.6394e+00, 0.0000e+00, 2.5629e+00],\n",
      "         [5.8323e+00, 5.7364e+00, 0.0000e+00, 4.3463e+00],\n",
      "         [6.2409e+00, 3.6138e+00, 0.0000e+00, 2.0767e+00],\n",
      "         [5.0229e+00, 3.5826e+00, 0.0000e+00, 1.8447e+00],\n",
      "         [3.8964e+00, 2.9503e+00, 0.0000e+00, 2.1923e+00],\n",
      "         [5.3489e+00, 2.1451e+00, 0.0000e+00, 3.3209e+00],\n",
      "         [8.1858e+00, 1.5025e+00, 0.0000e+00, 1.9491e+00],\n",
      "         [8.9314e+00, 3.7158e+00, 0.0000e+00, 1.6774e+00],\n",
      "         [6.4040e+00, 5.6761e+00, 0.0000e+00, 1.5647e+00],\n",
      "         [3.3613e+00, 5.9546e+00, 0.0000e+00, 3.3485e+00],\n",
      "         [4.6490e+00, 3.7196e+00, 0.0000e+00, 4.6941e+00],\n",
      "         [6.9544e+00, 7.5606e-01, 0.0000e+00, 8.9287e-01],\n",
      "         [7.2159e+00, 3.0207e+00, 0.0000e+00, 2.0080e+00],\n",
      "         [7.4525e+00, 4.4515e+00, 0.0000e+00, 2.7830e+00]],\n",
      "\n",
      "        [[1.4433e-01, 0.0000e+00, 0.0000e+00, 1.9459e+00],\n",
      "         [2.5701e+00, 0.0000e+00, 0.0000e+00, 9.1935e-01],\n",
      "         [3.0717e+00, 0.0000e+00, 0.0000e+00, 9.5156e-01],\n",
      "         [4.9492e+00, 0.0000e+00, 0.0000e+00, 2.3011e+00],\n",
      "         [7.8971e+00, 2.4020e-02, 0.0000e+00, 1.2273e+00],\n",
      "         [8.7003e+00, 2.7023e+00, 0.0000e+00, 1.1923e+00],\n",
      "         [7.7157e+00, 5.6977e+00, 0.0000e+00, 2.7020e+00],\n",
      "         [5.6408e+00, 5.9757e+00, 0.0000e+00, 2.5120e+00],\n",
      "         [3.7794e+00, 5.0527e+00, 0.0000e+00, 3.2036e+00],\n",
      "         [5.2837e+00, 3.2626e+00, 0.0000e+00, 4.1259e+00],\n",
      "         [8.4684e+00, 1.8992e+00, 0.0000e+00, 2.2406e+00],\n",
      "         [7.7742e+00, 3.3020e+00, 0.0000e+00, 2.4753e-01],\n",
      "         [5.5892e+00, 6.3511e+00, 0.0000e+00, 3.7454e+00],\n",
      "         [6.4847e+00, 5.2079e+00, 0.0000e+00, 4.2823e+00],\n",
      "         [6.9279e+00, 3.5894e+00, 0.0000e+00, 1.6501e+00],\n",
      "         [5.0165e+00, 4.3080e+00, 0.0000e+00, 1.9084e+00],\n",
      "         [5.0395e+00, 4.3106e+00, 0.0000e+00, 4.0775e+00],\n",
      "         [7.6439e+00, 2.7421e+00, 0.0000e+00, 2.9628e+00],\n",
      "         [9.0547e+00, 3.7695e+00, 0.0000e+00, 2.0344e+00],\n",
      "         [6.8876e+00, 5.5427e+00, 0.0000e+00, 1.3898e+00]],\n",
      "\n",
      "        [[1.6263e+00, 0.0000e+00, 0.0000e+00, 3.4141e+00],\n",
      "         [6.8599e+00, 0.0000e+00, 0.0000e+00, 1.3335e+00],\n",
      "         [6.6203e+00, 1.1996e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [4.1793e+00, 3.1240e+00, 0.0000e+00, 1.3396e+00],\n",
      "         [4.4341e+00, 3.1074e+00, 0.0000e+00, 3.8076e+00],\n",
      "         [7.5712e+00, 1.4499e+00, 0.0000e+00, 2.4807e+00],\n",
      "         [9.1582e+00, 2.8889e+00, 0.0000e+00, 1.4976e+00],\n",
      "         [6.8186e+00, 5.1920e+00, 0.0000e+00, 1.0995e+00],\n",
      "         [4.8570e+00, 6.8577e+00, 0.0000e+00, 4.6714e+00],\n",
      "         [5.3005e+00, 3.7869e+00, 0.0000e+00, 2.8017e+00],\n",
      "         [6.6029e+00, 3.3593e+00, 0.0000e+00, 3.2398e+00],\n",
      "         [8.2950e+00, 3.4730e+00, 0.0000e+00, 2.4859e+00],\n",
      "         [8.5354e+00, 5.2028e+00, 0.0000e+00, 2.5980e+00],\n",
      "         [7.8621e+00, 6.8621e+00, 0.0000e+00, 3.5607e+00],\n",
      "         [6.1115e+00, 6.5105e+00, 0.0000e+00, 2.7618e+00],\n",
      "         [4.1103e+00, 5.6602e+00, 0.0000e+00, 3.3103e+00],\n",
      "         [5.3032e+00, 3.9526e+00, 0.0000e+00, 4.3793e+00],\n",
      "         [6.9125e+00, 1.6154e+00, 0.0000e+00, 1.0651e+00],\n",
      "         [6.9055e+00, 3.6575e+00, 0.0000e+00, 2.4595e+00],\n",
      "         [7.4103e+00, 4.5094e+00, 0.0000e+00, 3.0084e+00]]], device='cuda:0',\n",
      "       grad_fn=<CudnnRnnBackward0>)\n",
      "torch.Size([5, 20, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.91, 0.2161535769701004)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.train()\n",
    "\n",
    "# New epoch --> fresh hidden state\n",
    "hidden = None   \n",
    "correct = 0\n",
    "for batch_idx in range(train_size):\n",
    "    if batch_idx == 1:\n",
    "        break\n",
    "    data, target = train_data[batch_idx]\n",
    "    data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).float().to(device)\n",
    "    optimizer.zero_grad()\n",
    "    if hidden is not None: hidden.detach_()\n",
    "\n",
    "    # Prediction, and tracking hidden states\n",
    "    # time_steps = data.shape[1]\n",
    "    logits, hidden, hidden_states = model(data, hidden)\n",
    "\n",
    "    print(hidden_states.shape, hidden_states)\n",
    "    # print(logits.shape, logits)\n",
    "    # print(hidden.shape, hidden)\n",
    "\n",
    "    # RNN has a bijection between \n",
    "    # print(data.shape, target.shape)\n",
    "\n",
    "    # Do this thing below, and then normalize by that, like divide every\n",
    "    # gradient step by the L2 norm.\n",
    "    \n",
    "    # loss_arr = []\n",
    "    # for i in range(target.shape[1]):\n",
    "    #     loss_arr.append(criterion(logits[:, i:i+1, :], target[:,i:i+1,:]) * torch.tensor(1, device=device)) # This isn't the 5 step training loop, and will yield problems if I do backward, step all inside the loop\n",
    "\n",
    "    loss = criterion(logits, target) # It doesn't do anything really special\n",
    "    # It just has a 1-1 mapping between data and target, all at once.\n",
    "    # And then it later on does a topological sort, rooted at loss.\n",
    "    loss.backward() # Calculates all gradients involved (anything that has autograd=True)\n",
    "    # And adds the grad dL/dw_i\n",
    "    print(hidden_states.grad.shape)\n",
    "    hidden_states_norm = torch.norm(hidden_states, p=2, dim=-1)\n",
    "    hidden_states_norm_avg = torch.mean(hidden_states_norm, dim=0)\n",
    "\n",
    "    plt.plot(range((hidden_states_norm_avg.detach().cpu().shape[0])), list(hidden_states_norm_avg.detach().cpu()))\n",
    "    plt.show()\n",
    "\n",
    "    optimizer.step() # This just steps all of those things, but carefully (i.e. following\n",
    "    # a stepping algorithm, like AdamW)\n",
    "\n",
    "    \n",
    "    pred = (torch.sigmoid(logits) > 0.5)\n",
    "    correct += (pred == target.byte()).int().sum().item()/total_values_in_one_chunck\n",
    "    \n",
    "correct, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(loss_arr)), loss_arr)\n\u001b[32m      4\u001b[39m plt.show()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(loss_arr)), loss_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_at_time_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m T = \u001b[32m10\u001b[39m \u001b[38;5;66;03m# Example total time steps\u001b[39;00m\n\u001b[32m      2\u001b[39m t = \u001b[32m5\u001b[39m  \u001b[38;5;66;03m# Fixed time step for loss E_t\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m L_t = \u001b[43mloss_at_time_t\u001b[49m[t]\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# x_k are the hidden states for k=1 to T\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# W_rec is the recurrent weight matrix (your theta)\u001b[39;00m\n\u001b[32m      6\u001b[39m individual_term_k_array = []\n",
      "\u001b[31mNameError\u001b[39m: name 'loss_at_time_t' is not defined"
     ]
    }
   ],
   "source": [
    "T = 10 # Example total time steps\n",
    "t = 5  # Fixed time step for loss E_t\n",
    "L_t = loss_at_time_t[t]\n",
    "# x_k are the hidden states for k=1 to T\n",
    "# W_rec is the recurrent weight matrix (your theta)\n",
    "individual_term_k_array = []\n",
    "\n",
    "# Detach x_{k-1} for the immediate derivative calculation: ^+x_k / \n",
    "for k in range(1, t + 1):\n",
    "    # --- Step A: Calculate E_t / x_k ---\n",
    "    grad_E_t_to_x_k = torch.autograd.grad(\n",
    "        outputs=L_t, \n",
    "        inputs=x_k_states[k], \n",
    "        retain_graph=True, \n",
    "        allow_unused=True\n",
    "    )[0] \n",
    "    \n",
    "    # --- Step B: Calculate ^+x_k /  (immediate derivative) ---\n",
    "    # Re-run the forward pass of the cell for step k, but with x_{k-1} detached\n",
    "    if k > 1:\n",
    "        x_k_minus_1_detached = x_k_states[k-1].detach()\n",
    "    else:\n",
    "        x_k_minus_1_detached = initial_state\n",
    "        \n",
    "    # Recalculate x_k using the detached previous state\n",
    "    x_k_immediate = rnn_cell_forward(x_k_minus_1_detached, input_at_k, W_rec)\n",
    "    \n",
    "    grad_x_k_immediate_to_W_rec = torch.autograd.grad(\n",
    "        outputs=x_k_immediate, \n",
    "        inputs=W_rec, \n",
    "        retain_graph=True, \n",
    "        allow_unused=True\n",
    "    )[0]\n",
    "    \n",
    "    # --- Step C: Combine the components for Term_k ---\n",
    "    # The exact combination depends on shapes (matrix multiplication, element-wise, etc.)\n",
    "    # Conceptually, this is the final Term_k:\n",
    "    term_k = torch.matmul(grad_E_t_to_x_k.T, grad_x_k_immediate_to_W_rec) \n",
    "    \n",
    "    individual_term_k_array.append(term_k.detach().clone())\n",
    "\n",
    "# The array 'individual_term_k_array' now contains Term_k for k=1 to t.\n",
    "# sum(individual_term_k_array) should equal the total gradient E_t / ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
